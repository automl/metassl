#######################################
###       Logging Configuration     ###
#######################################
expt:
  project_name:     'metassl'
  session_name:     'resnet50_metassl_imagenet'
  experiment_name:  'default_config'
  job_name:         '001'
  save_model:       True                                      # save the model to disc
  save_model_freq:  5                                         # save model frequency in # of epochs
  checkpoint_path:  "/home/ferreira/workspace/experiments/metassl/checkpoint_0016.pth.tar"    # resumes training if model with same config exists
  print_freq:       10                                        # print frequency
  gpu:              null                                      # GPU ID to train on (if not distributed)
  multiprocessing_distributed:  True                          # Use multi-processing distributed training to launch N processes per node, which has N GPUs.
                                                              # This is the fastest way to use PyTorch for either single node or multi node data parallel training'
  dist_backend:                 'nccl'                        # distributed backend
  dist_url:                     'tcp://localhost:10001'       # url used to set up distributed training
  workers:                      32                            # number of data loading workers
  rank:                         0                             # node rank for distributed training
  world_size:                   1                             # number of nodes for distributed training
  eval_freq:                    10                            # every eval_freq iteration will the model be evaluated
  seed:                         123                           # random seed of numpy and torch

#######################################
###     Training Configuration      ###
#######################################
train:
  batch_size: 256                         # in distributed setting this is the total batch size, i.e. batch size = individual bs / number of GPUs
  epochs:     100                         # epochs to train
  start_epoch: 0                          # start training at epoch n

finetuning:
  epochs: 10
  optim:
    optimizer:        "lars"                # optimizer type, options: adam adamW rmsprop adabelief sgd
    schedule:         "cosine"              # learning rate schedule, options: 'cosine' 'cosineW' 'plateau' 'step' 'const' 'cosineWarm' 'exponential' 'None' (BOHO case)
    warmup:           1000                  # 0 (turned off) or higher (e.g. 1000 ~ 5 epochs at batch size 256 on CIFAR100)
    factor:           1.0                   # noam factor
    weight_decay:     0.0001

    lr_low:           0.0                   # the minimum LR (eta_min) for schedulers 'cosine', 'cosineWarm' and 'plateau'
    lr_high:          0.02                  # the default LR for all schedulers, in case of const LR schedule: set lr_high=lr_low
    clip_grad:        False                 # gradient cliping, options: 0.1 ... 100, False
    scheduler_epochs: 100                   # T_max or T_0 in 'cosine' and 'cosineWarm' scheduler, in case of 'step' scheduler this denotes when scheduler should step


#######################################
###       Model Configuration       ###
#######################################
model:
  model_type: "resnet50"                  # supported models: all torchvision ResNets
  seed:       123


#######################################
###     Optimizer Configuration     ###
#######################################
optim:
  optimizer:        "sgd"                 # optimizer type, options: adam adamW rmsprop adabelief sgd
  schedule:         "cosine"              # learning rate schedule, options: 'cosine' 'cosineW' 'plateau' 'step' 'const' 'cosineWarm' 'exponential' 'None' (BOHO case)
  warmup:           1000                  # 0 (turned off) or higher (e.g. 1000 ~ 5 epochs at batch size 256 on CIFAR100)
  factor:           1.0                   # noam factor
  weight_decay:     0.0001
  momentum:         0.9                   # momentum of SGD solver

  lr_low:           0.0                   # the minimum LR (eta_min) for schedulers 'cosine', 'cosineWarm' and 'plateau'
  lr_high:          0.05                  # the default LR for all schedulers, in case of const LR schedule: set lr_high=lr_low
  clip_grad:        False                 # gradient cliping, options: 0.1 ... 100, False
  scheduler_epochs: 100                   # T_max or T_0 in 'cosine' and 'cosineWarm' scheduler, in case of 'step' scheduler this denotes when scheduler should step


#######################################
###       Data Configuration        ###
#######################################
data:
  seed:     123
  dataset:  'ImageNet'                    # supported datasets: CIFAR10, CIFAR100, ImageNet
  data_dir: '/home/ferreira/workspace/data/metassl'


#######################################
###       SimSiam Specific          ###
#######################################
simsiam:
  dim: 2048                                  # feature dimension (default: 2048)
  pred_dim: 512                              # hidden dimension of the predictor (default: 512)
  fix_pred_lr:   True                         # Fix learning rate for the predictor



