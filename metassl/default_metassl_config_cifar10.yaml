#######################################
###       Logging Configuration     ###
#######################################
expt:
  expt_name:                    'testing-cifar10'
  expt_mode:                    'CIFAR10'
  save_model:                   True                          # save the model to disc
  save_model_frequency:         100                           # save model frequency in # of epochs
  ssl_model_checkpoint_path:    null                          # resumes training if model with same config exists. e.g.: "experiments/<EXPERIMENT-NAME>/checkpoint_0099.pth.tar"
  target_model_checkpoint_path: "experiments"                 #
  print_freq:                   10                            # print frequency
  gpu:                          null                          # GPU ID to train on (if not distributed)
  multiprocessing_distributed:  True                          # Use multi-processing distributed training to launch N processes per node, which has N GPUs.
  # This is the fastest way to use PyTorch for either single node or multi node data parallel training'
  dist_backend:                 'nccl'                        # distributed backend
  dist_url:                     'tcp://localhost:10001'       # url used to set up distributed training
  workers:                      8                             # number of data loading workers
  rank:                         0                             # node rank for distributed training
  world_size:                   1                             # number of nodes for distributed training
  eval_freq:                    10                            # every eval_freq iteration will the model be evaluated
  seed:                         123                           # random seed of numpy and torch
  evaluate:                     False                         # evaluate model on validation set once and terminate
  is_non_grad_based:            False                         # Set this flag to run default SimSiam or BOHB runs
  warmup_epochs:                10                            # Denotes the number of epochs that we only pre-train without finetuning afterwards; warmup is turned off when set to 0; we use a linear incremental schedule during warmup
  warmup_multiplier:            1.                            # A factor that is multiplied with the pretraining lr used in the linear incremental learning rate scheduler during warmup. The final lr is multiplier * pre-training lr
  warmup_both:                  False                         # Whether backbone and head should be both warmed up.
  use_fix_aug_params:           False                         # Use this flag if you want to try out specific aug params (e.g., from a best BOHB config). Default values will be overwritten then without crashing other experiments.
  data_augmentation_mode:       'default'                     # Select which data augmentation to use. Default is for the standard SimSiam setting and for parameterize aug setting. choices=['default', 'probability_augment', 'rand_augment']
  write_summary_frequency:      10                             # Specifies, after how many batches the TensorBoard summary writer should flush new data to the summary object.
  wd_decay_pt:                  False                         # use weight decay decay (annealing) during pre-training? (default: True)
  wd_decay_ft:                  False                         # use weight decay decay (annealing) during fine-tuning? (default: True)
  run_knn_val:                  False                         # activate knn evaluation during training (default: False)

#######################################
###     Training Configuration      ###
#######################################
train:
  batch_size:       512                   # in distributed setting this is the total batch size, i.e. batch size = individual bs / number of GPUs
  epochs:           800                   # number of pre-training epochs
  start_epoch:      0                     # start training at epoch n
  optimizer:        "sgd"                 # optimizer type, options: adam adamW rmsprop adabelief sgd
  schedule:         "cosine"              # learning rate schedule, options: 'cosine' 'cosineW' 'plateau' 'step' 'const' 'cosineWarm' 'exponential' 'None' (BOHO case)
  weight_decay:     0.0005
  momentum:         0.9                   # momentum of SGD solver
  lr:               0.03                  # the default LR for all schedulers, in case of const LR schedule: set lr_high=lr_low
  clip_grad:        False                 # gradient cliping, options: 0.1 ... 100, False
  scheduler_epochs: 100                   # T_max or T_0 in 'cosine' and 'cosineWarm' scheduler, in case of 'step' scheduler this denotes when scheduler should step
  val_freq:         10                    # validate with knn
  wd_start:         1.e-3                 # Upper value of WD Decay. Only used when wd_decay is True.
  wd_end:           1.e-6                 # Lower value of WD Decay. Only used when wd_decay is True.

finetuning:
  batch_size:        256                   # in distributed setting this is the total batch size, i.e. batch size = individual bs / number of GPUs
  epochs:            100                   # number of finetuning epochs
  start_epoch:       0                     # start training at epoch n
  optimizer:         "sgd"                 # optimizer type, options: lars adam adamW rmsprop adabelief sgd
  schedule:          "cosine"              # learning rate schedule, options: 'cosine' 'cosineW' 'plateau' 'step' 'const' 'cosineWarm' 'exponential' 'None' (BOHO case)
  weight_decay:      0.
  momentum:          0.9                   # momentum of SGD solver
  lr:                30                   # the default LR
  clip_grad:         False                 # gradient cliping, options: 0.1 ... 100, False
  scheduler_epochs:  100                   # T_max or T_0 in 'cosine' and 'cosineWarm' scheduler, in case of 'step' scheduler this denotes when scheduler should step
  valid_size:        0.0                   # If valid_size > 0, pick some images from the trainset to do evaluation on. If valid_size=0 evaluation is done on the testset.
  data_augmentation: "none"               # Select if and how finetuning gets augmented. choices=['none', 'p_probability_augment_pt', 'p_probability_augment_ft', 'p_probability_augment_1-pt']
  wd_start:          1.e-3                 # Upper value of WD Decay. Only used when wd_decay is True.
  wd_end:            1.e-6                 # Lower value of WD Decay. Only used when wd_decay is True.
  use_alternative_scheduler: False         # Use the learning rate scheduler from the baseline codebase

#######################################
###       Model Configuration       ###
#######################################
model:
  model_type:  "resnet18"                # supported models: all torchvision ResNets
  seed:        123
  turn_off_bn: False                     # turns off all batch norm instances in the model
  arch:        "baseline_resnet"              # Select architecture for CIFAR10; choices=["our_resnet", "tv_resnet", "baseline_resnet"]


#######################################
###       Data Configuration        ###
#######################################
data:
  seed:                     123
  dataset:                  'CIFAR10'                    # supported datasets: CIFAR10, CIFAR100, ImageNet
  dataset_percentage_usage: 100         # Indicates what percentage of the data is used for the experiments.


#######################################
###       SimSiam Specific          ###
#######################################
simsiam:
  dim:         2048                                  # feature dimension (default: 2048)
  pred_dim:    512                              # hidden dimension of the predictor (default: 512)
  fix_pred_lr: True                        # Fix learning rate for the predictor
  use_baselines_loss: False                # Set this flag to use the loss from the baseline code (PT)

########################################
####       Learn Augmentations       ###
########################################
learnaug:
    type:     "colorjitter"             # Define which type of learned augmentation to use.

#######################################
###       BOHB Specific             ###
#######################################
bohb:
  run_id:           "default_BOHB"
  seed:             123
  n_iterations:     10                          # How many BOHB iterations
  min_budget:       5
  max_budget:       20
  budget_mode:      "epochs"                    # Choose your desired fidelity between 'epochs' and 'data'
  eta:              2
  configspace_mode: "color_jitter_strengths"    # Define which configspace to use
                                                # choices=["imagenet_probability_augment",
                                                # "cifar10_probability_augment", "color_jitter_strengths"]
  nic_name:         "lo"                        # Define the network interface to use > local: "lo", cluster: "eth0"
  port:             0
  worker:           False                       # Make this execution a worker server, action="store_true"
  warmstarting:     False
  warmstarting_dir: None
  test_env:         False

#######################################
###       NEPS Specific             ###
#######################################
neps:
  is_neps_run:    False                                 # Set this flag to run a NEPS experiment.
  config_space:    'parameterized_cifar10_augmentation'  # Define which configspace to use. choices=['parameterized_cifar10_augmentation', 'probability_augment']

